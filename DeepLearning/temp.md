# 딥러닝 이해 7장 연습문제   

---

1. 풀 배치와 온라인 학습을 비교해보자. 각 방법의 장점은 무엇인가?
- 풀 배치 학습은 모든 훈련 샘플을 처리한 후에 가중치를 변경하는 방법으로, 안정적이지만 시간이 오래 걸린다. 온라인 학습은 확률적 경사 하강법이라고도 하며, 이는 훈련 샘플 중에서 무작위로 하나를 골라서 학습을 수행하는 방식이다. 이 방식은 빠르고 계산하기 쉽다는 장점이 있다.

2. 우리가 다루는 데이터의 크기가 아주 크다고 하자. 이 데이터를 사용하여 경사 하강법을 수행할 때는 배치 크기를 어떻게 하는 것이 좋을까?
- 데이터의 크기가 아주 클 경우에는 훈련 샘플을 작은 배치들로 분리시켜 학습을 수행해야 한다.

4. 신경망에서 하이퍼 매개변수란 무엇이고, 어떤 값들이 하이퍼 매개변수인가?
- 하이퍼 매개변수란 신경망 모델의 가중치나 바이어스와는 다르게, 개발자가 모델에 대하여 임의로 결정하는 값이다. 은닉층의 개수, 유닛 개수, 학습률, 모멘텀, 미니 배치 크기, 에포크 수 등이 여기에 속한다.

5. 신경망에서 최적의 하이퍼 매개변수를 찾는 방법을 설명해보자.
- 그리드 검색을 사용해 최적의 값을 검색할 수 있다. 이는 각 하이퍼 매개변수에 대하여 몇 개의 값을 지정하면 이 중에서 가장 좋은 조합을 찾아주는 알고리즘이다.

6. 학습률이 작으면 어떤 단점이 있는가? 반대로 큰 학습률의 문제점은 무엇인가? 학습률을 어떻게 결정하는 것이 좋을까?
- 학습률이 너무 낮다면 아주 느리게 학습이 이루어지며, 그만큼 수렴되는 시간도 늦어진다. 또 지역 최소화에 빠질 위험도 높아진다. 반대로 학습률이 너무 높으면 오버슈팅이 일어나고 불안정해지면 발산할 수 있다. 학습률 설정에 가장 좋은 방법은 손실 함수의 모양에 따라서 적응적으로 학습률을 변경하는 방법이다.

7. 구글의 플레이그라운드를 이용하여 높은 학습률이 모델에 어떠한 영향을 주는지 살펴보자.
- 적당히 높은 학습률은 학습 시간을 단축시켜주지만 너무 높은 학습률은 결과가 불안정하게 나오는 것을 확인할 수 있다.

9. 케라스를 이용하여 은닉층이 2개인 MLP를 생성해보자. 은닉층이 2개인 MLP를 이용하여, MNIST 숫자들을 처리해본다. 은닉층이 하나일 때와 차이가 있는가?
- 은닉층이 하나일 때 보다 시간은 오래 걸리지만 정확도는 높아진다.

10. 본문에는 MNIST 데이터 세트를 분류하는 소스가 포함되어 있다. 이 소스에서 최적화 방법을 “Adam”, “RMSprop”, “Adadelta”, “Adagrad”으로 변경하면서 성능을 측정하여 보자. MNIST 데이터 세트의 경우, 어떤 방법이 가장 좋은 성능을 보이는가?
- 실행 시간은 Adam, Adadelta, Adagrad가 RMSprop보다 빠르고, 정확도는 Adam = RMSprop > Adagrad > Adadelta 순으로 높다. Adam과 RMSprop이 좋은 성능을 보인다.

---

# 딥러닝 이해 8장 연습문제

1. 심층 신경망에서 은닉층이 하는 역할은 무엇인가?
- 여러 개의 은닉층 중에서 앞단은 경계선(에지)과 같은 저급 특징들을 추출하고 뒷단은 코너와 같은 고급 특징들을 추출한다.

2. 은닉층이 많아지면 출력층에서 계산된 그래디언트가 역전파되다가 값이 점점 작아져서 없어지는 문제점을 무엇이라고 하는가? 문제의 원인은 무엇이었는가?
- 그래디언트 소실 문제라고한다. 이는 시그모이드 활성화 함수의 특성상, 아주 큰 양수나 음수가 들어오면 출력이 포화되어 거의 0이 되기 때문이다.

3. 최근에 큰 인기를 끌고 있는 활성화 함수는 무엇인가?
- 최근의 신경망에서는 출력층에 소프트맥스(softmax) 함수를 많이 사용한다.

4. 교차 엔트로피란 무엇인가? 교차 엔트로피를 손실 함수로 사용하려면 목표 출력과 실제 출력을 어떻게 만들어야 하는가?
- 교차 엔트로피는 2개의 확률분포 간의 거리를 측정한 것이다. 목표 출력의 확률 분포를 원-핫 인코딩을 사용하고, 실제 출력을 확률 분포를 만들기 위해 소프트맥스 함수를 출력층의 활성화 함수로 사용해 만든다.

5. 심층 신경망에서 과잉 적합과 과소 적합이란 무엇이며 이것은 어떻게 방지할 수 있는가?
- 과잉 적합은 지나치게 훈련 데이터에 특화돼 실제 적용 시 좋지 못한 결과가 나오는 것을 말하고, 이는 많은 훈련 데이터를 사용해 방지할 수 있다.
- 과소 적합은 신경망 모델이 충분히 훈련되지 않는 것이다. 이는 파라미터가 더 많은 모델을 선택하고 조기종료 시점까지 충분히 학습해 방지할 수 있다.

6. 심층 신경망에서 초기 가중치는 어떻게 설정하는 것이 좋은가? 최악의 가중치는 무엇인가?
- 가중치의 초기값은 난수로 결정하는 것이 좋다. 최악의 가중치는 초기값을 0으로 주게 될 경우로, 이 경우오차 역전파가 제대로 되지 않는다.

7. 훈련 데이터를 작은 배치들로 분리시켜서 하나의 배치가 끝날 때마다 학습을 수행하는 방법을 무엇이라고 하는가?
- 미니 배치라고 한다.
